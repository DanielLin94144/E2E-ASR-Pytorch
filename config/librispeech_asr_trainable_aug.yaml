data:
  corpus:                                 # Pass to dataloader
    # The following depends on corpus
    path: '/home/darong/frequent_data/LibriSpeech'
    name: 'LibriSpeech'
    train_split: ['train-clean-100']
    dev_split: ['dev-clean']
    bucketing: True
    batch_size: 4

  audio:                                  # Pass to audio transform
    feat_type: 'fbank'
    feat_dim:  40 
    apply_cmvn: False
    delta_order: 2                      # 0: do nothing, 1: add delta, 2: add delta and accelerate
    delta_window_size: 2
    frame_length: 25 # ms
    frame_shift: 10 # ms
    ref_level_db: 20
    min_level_db: -100
    preemphasis_coeff: 0.97
    time_aug: False
  text:
    mode: 'character'                     # 'character'/'word == phone'/'subword'
    vocab_file: 'corpus/librispeech_char.txt'

hparas:                                   # Experiment hyper-parameters
  valid_step: 2000
  max_step: 200000
  tf_start: 1.0
  tf_end: 1.0
  tf_step: 150000
  optimizer: 'Adadelta'
  lr: 1.0
  eps: 0.00000001                          # 1e-8
  lr_scheduler: 'self_defined'                    # 'fixed'/'warmup'
  curriculum: 0
  val_mode: 'wer'
  early_stopping: False
  label_smoothing: True

model:                                    # Model architecture
  ctc_weight: 0.5                         # Weight for CTC loss
  encoder:
    vgg: 5                     # 4x reduction on time feature extraction ( 5: VGG + layer normalization)
    vgg_freq: -1
    vgg_low_filt: -1
    module: 'LSTM'                        # 'LSTM'/'GRU'/'Transformer'
    bidirection: True
    dim: [128, 128, 128, 128, 128]
    dropout: [0.3, 0.3,0.3,0.3,0.3]
    layer_norm: [False, False, False, False, False]
    proj: [True,True,True,True, True]           # Linear projection + Tanh after each rnn layer
    sample_rate: [1,1, 1,1,1]
    sample_style: 'drop'                  # 'drop'/'concat'
  attention:
    mode: 'loc'                           # 'dot'/'loc'
    dim: 300
    num_head: 1
    v_proj: False                         # if False and num_head>1, encoder state will be duplicated for each head
    temperature: 0.5                      # scaling factor for attention
    loc_kernel_size: 100                  # just for mode=='loc'
    loc_kernel_num: 10                    # just for mode=='loc'
  decoder:
    module: 'LSTM'                        # 'LSTM'/'GRU'/'Transformer'
    dim: 128
    layer: 2
    dropout: 0

augmentation:
  type: 1 # 1: train aug 2: use pretrain aug module 3:previous specaug 4: no aug(only means no aug on spectrogram)

  specaug: # arg meaning please refer to src/audio.py: class Augment # only valid if type==3
    T: 40
    num_masks: 1
    replace_with_zero: False
    F: 27

  trainable_aug: # arg meaning please refer to src/augmentation.py: class TrainableAugment # only valid if type==1,2
    model:
      T_num_masks: 1
      F_num_masks: 1
      # noise_dim: None
      dim: [3, 3]
      replace_with_zero: False
      width_init_bias: -3.
    optimizer:
      optimizer: 'AdamW'
      lr: 1.0
      amsgrad: True